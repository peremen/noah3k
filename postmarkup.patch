--- postmarkup.py.orig	2010-08-30 00:19:00.000000000 +0900
+++ postmarkup.py	2010-08-31 04:23:35.000000000 +0900
@@ -10,6 +10,7 @@
 import re
 from urllib import quote, unquote, quote_plus, urlencode
 from urlparse import urlparse, urlunparse
+import random, sys
 
 pygments_available = True
 try:
@@ -53,7 +54,7 @@
     if match is None:
         return ""
     excerpt = match.group(0)
-    excerpt = excerpt.replace(u'\n', u"<br/>")
+    excerpt = excerpt.replace(u'\n', u"<br />")
     return _re_remove_markup.sub("", excerpt)
 
 def strip_bbcode(bbcode):
@@ -64,10 +65,10 @@
 
     """
 
-    return u"".join([t[1] for t in PostMarkup.tokenize(bbcode) if t[0] == PostMarkup.TOKEN_TEXT])
+    return u"".join([t[1] for t in PostMarkup().tokenize(bbcode) if t[0] == PostMarkup.TOKEN_TEXT])
 
 
-def create(include=None, exclude=None, use_pygments=True, **kwargs):
+def create(include=None, exclude=None, use_pygments=True, tag_factory=None, **kwargs):
 
     """Create a postmarkup object that converts bbcode to XML snippets. Note
     that creating postmarkup objects is _not_ threadsafe, but rendering the
@@ -84,7 +85,7 @@
 
     """
 
-    postmarkup = PostMarkup()
+    postmarkup = PostMarkup(tag_factory=tag_factory)
     postmarkup_add_tag = postmarkup.tag_factory.add_tag
 
     def add_tag(tag_class, name, *args, **kwargs):
@@ -306,7 +307,7 @@
             return u""
 
         if self.domain:
-            return u'<a href="%s">'%self.url
+            return u'<a href="%s">'%PostMarkup.standard_replace(self.url)
         else:
             return u""
 
@@ -345,13 +346,14 @@
 
     def render_open(self, parser, node_index):
         if self.params:
-            return u'<blockquote><em>%s</em><br/>'%(PostMarkup.standard_replace(self.params))
+            quote_id = random.randint(0, sys.maxint)
+            return u'<em><a class="quote" href="#" onclick="$(\'#quote%s\').toggle();">%s</a></em><blockquote id="quote%s">'%(quote_id, PostMarkup.standard_replace(self.params), quote_id)
         else:
             return u'<blockquote>'
 
 
     def render_close(self, parser, node_index):
-        return u"</blockquote>"
+        return u"</blockquote>"
 
 
 class SearchTag(TagBase):
@@ -405,7 +407,7 @@
 
         formatter = HtmlFormatter(linenos=self.line_numbers, cssclass="code")
         hcontents = highlight(contents, lexer, formatter)
-        hcontents = hcontents.strip().replace('\n', '<br>')
+        hcontents = hcontents.strip().replace('\n', '<br />')
 
         return hcontents
 
@@ -420,7 +422,7 @@
 
         contents = _escape_no_breaks(self.get_contents(parser))
         self.skip_contents(parser)
-        return '<div class="code"><pre>%s</pre></div>' % contents.replace("\n", "<br/>")
+        return '<div class="code"><pre>%s</pre></div>' % contents.replace("\n", "<br />")
 
 
 class ImgTag(TagBase):
@@ -778,7 +780,7 @@
     standard_replace = MultiReplace({   u'<':u'&lt;',
                                         u'>':u'&gt;',
                                         u'&':u'&amp;',
-                                        u'\n':u'<br/>'})
+                                        u'\n':u'<br />'})
 
     standard_unreplace = MultiReplace({  u'&lt;':u'<',
                                          u'&gt;':u'>',
@@ -802,11 +804,10 @@
     _re_quote_end = re.compile(u'\"|\]', re.UNICODE)
 
     # I tried to use RE's. Really I did.
-    @classmethod
-    def tokenize(cls, post):
+    def tokenize(self, post):
 
-        re_end_eq = cls._re_end_eq
-        re_quote_end = cls._re_quote_end
+        re_end_eq = self._re_end_eq
+        re_quote_end = self._re_quote_end
 
         text = True
         pos = 0
@@ -817,9 +818,23 @@
             except AttributeError:
                 return -1
 
+        def _tag_name(token):
+            token = token[1:-1].lstrip().lower()
+            if not token:
+                return u''
+            if token[0] == '/':
+                token = token[1:]
+            if u'=' in token:
+                token = token.split(u'=',1)[0]
+            if u' ' in token:
+                return token.split(u' ',1)[0]
+            return token
+
         TOKEN_TAG, TOKEN_PTAG, TOKEN_TEXT = range(3)
 
         post_find = post.find
+        supported_tags = self.get_supported_tags()
+
         while True:
 
             brace_pos = post_find(u'[', pos)
@@ -846,7 +861,11 @@
                 continue
 
             if post[end_pos] == ']':
-                yield TOKEN_TAG, post[pos:end_pos+1], pos, end_pos+1
+                token = post[pos:end_pos+1]
+                token_type = TOKEN_TAG
+                if _tag_name(token) not in supported_tags:
+                    token_type = TOKEN_TEXT
+                yield token_type, token, pos, end_pos+1
                 pos = end_pos+1
                 continue
 
@@ -858,8 +877,19 @@
                     if post[end_pos] != '"':
                         end_pos = post_find(u']', end_pos+1)
                         if end_pos == -1:
+                            yield TOKEN_TEXT, post[pos:], pos, len(post)
                             return
-                        yield TOKEN_TAG, post[pos:end_pos+1], pos, end_pos+1
+                        has_another_open = post_find(u'[', pos+1)
+                        if has_another_open != -1 and has_another_open < end_pos:
+                            if _tag_name(post[pos:end_pos+1]) not in supported_tags:
+                                yield TOKEN_TEXT, post[pos:has_another_open], pos, has_another_open
+                                pos = has_another_open
+                                continue
+                        token = post[pos:end_pos+1]
+                        token_type = TOKEN_TAG
+                        if _tag_name(token) not in supported_tags:
+                            token_type = TOKEN_TEXT
+                        yield token_type, token, pos, end_pos+1
                     else:
                         end_pos = find_first(post, end_pos, re_quote_end)
                         if end_pos==-1:
@@ -871,9 +901,17 @@
                             end_pos = post_find(u']', end_pos+1)
                             if end_pos == -1:
                                 return
-                            yield TOKEN_PTAG, post[pos:end_pos+1], pos, end_pos+1
+                            token = post[pos:end_pos+1]
+                            token_type = TOKEN_PTAG
+                            if _tag_name(token) not in supported_tags:
+                                token_type = TOKEN_TEXT
+                            yield token_type, token, pos, end_pos+1
                         else:
-                            yield TOKEN_TAG, post[pos:end_pos+1], pos, end_pos
+                            token = post[pos:end_pos+1]
+                            token_type = TOKEN_TAG
+                            if _tag_name(token) not in supported_tags:
+                                token_type = TOKEN_TEXT
+                            yield token_type, token, pos, end_pos
                     pos = end_pos+1
                 except IndexError:
                     return
@@ -1164,16 +1202,15 @@
 
             elif tag_type == TOKEN_TAG:
                 tag_token = tag_token[1:-1].lstrip()
-                if ' ' in tag_token:
+                if '=' in tag_token:
+                    tag_name, tag_attribs = tag_token.split(u'=', 1)
+                    tag_attribs = tag_attribs.strip()
+                elif ' ' in tag_token:
                     tag_name, tag_attribs = tag_token.split(u' ', 1)
                     tag_attribs = tag_attribs.strip()
                 else:
-                    if '=' in tag_token:
-                        tag_name, tag_attribs = tag_token.split(u'=', 1)
-                        tag_attribs = tag_attribs.strip()
-                    else:
-                        tag_name = tag_token
-                        tag_attribs = u""
+                    tag_name = tag_token
+                    tag_attribs = u""
             else:
                 tag_token = tag_token[1:-1].lstrip()
                 tag_name, tag_attribs = tag_token.split(u'=', 1)
